{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMW9l0F2Xi4RECR/NIGoCbQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arup3201/AOTS/blob/main/notebooks/Artificial_Traffic_Management_ML_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Traffic Management - ML Model"
      ],
      "metadata": {
        "id": "UjP0jDMnOEUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "AtqH5ZCdPBUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPfj3SEzPHtv",
        "outputId": "05ae4d26-bc50-4d79-f2e7-9028d9685209"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image\n",
        "from xml.dom import minidom\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "PffOpxnJPIeT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variables**"
      ],
      "metadata": {
        "id": "4M-2axWNQVGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_DIR = '/content/dataset/train'\n",
        "VALIDATION_DIR = '/content/dataset/valid'\n",
        "TEST_DIR = '/content/dataset/test'\n",
        "CLASS_NAMES = ['car', 'bus', 'truck']\n",
        "S = 7\n",
        "B = 2\n",
        "C = len(CLASS_NAMES)\n",
        "IMAGE_DIMS = [448, 448]\n",
        "NUM_CHANNELS = 3\n",
        "BATCH_SIZE = 16\n",
        "LAMBDA_COORD = 5\n",
        "LAMBDA_NOOBJ = 0.5\n",
        "\n",
        "EPOCHS = 30\n",
        "TRAIN_STEPS = 160\n",
        "VAL_STEPS = 60\n",
        "\n",
        "dataset_params = {'class_names': CLASS_NAMES,\n",
        "                  'S': S,\n",
        "                  'B': B,\n",
        "                  'dims': IMAGE_DIMS,\n",
        "                  'n_channels': NUM_CHANNELS,\n",
        "                  'batch_size': BATCH_SIZE,\n",
        "                  'shuffle': True,\n",
        "                  'transform': None}\n",
        "\n",
        "model_params = {'S': S,\n",
        "                'B': B,\n",
        "                'C': C,\n",
        "                'img_shape': IMAGE_DIMS}\n",
        "\n",
        "loss_params = {'S': S,\n",
        "               'B': B,\n",
        "               'C': C,\n",
        "               'lambda_coord': LAMBDA_COORD,\n",
        "               'lambda_noobj': LAMBDA_NOOBJ}"
      ],
      "metadata": {
        "id": "sBHeG1PiQZLu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "FZULTkTfPL5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Data"
      ],
      "metadata": {
        "id": "v5Uvgo76PnwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r dataset/\n",
        "!wget https://universe.roboflow.com/ds/Zq7rUF1khY?key=UXLLVXz75H\n",
        "\n",
        "!mkdir dataset\n",
        "!unzip -q Zq7rUF1khY?key=UXLLVXz75H -d dataset/\n",
        "!rm Zq7rUF1khY?key=UXLLVXz75H"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhNvtVWNPPDN",
        "outputId": "ea6d1946-3056-4552-e160-aa9163a81225"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'dataset/': No such file or directory\n",
            "--2023-12-01 12:32:47--  https://universe.roboflow.com/ds/Zq7rUF1khY?key=UXLLVXz75H\n",
            "Resolving universe.roboflow.com (universe.roboflow.com)... 151.101.65.195, 151.101.1.195\n",
            "Connecting to universe.roboflow.com (universe.roboflow.com)|151.101.65.195|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://storage.googleapis.com/roboflow-platform-exports/pwYAXv9BTpqLyFfgQoPZ/peHMC7FkjCfPPfu0wVn3/2/voc.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=481589474394-compute%40developer.gserviceaccount.com%2F20231201%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20231201T123247Z&X-Goog-Expires=901&X-Goog-SignedHeaders=host&X-Goog-Signature=2de2cb741bb179b2c80f64e1ee4cc344f8224183ed5afbe17295a5af5c7d513bdd64ef0e421976cd01ebe0e6f914fa5f81bacb7ea66ba4c8dd2182ca7f94d06bb5fd0e0fd1a1565aae0aca758ba94feff17f5a9fe0d7c67d70d21b0d8b2d33dcbb58f9d357583e56f8fc9c495cc6963157205349393cd2a592e2178804c5d32cfa910d3f01efd13b3f518f7f68b9a8aab3ff27ce4783192632f0d5c61b2908cbd0bc8c281b8358ad590e153a25e3d653389e045f0e1e4bcaf53a2c28571707048fa9104da3c7f082a3cf07dd1072124b6caa7aa4b7281d5b8a94c613a1563300ac4f88a2c64e5cbf77ef430d3b9b4ff88f8ae9a12f8b05f9478494c2522e6e57 [following]\n",
            "--2023-12-01 12:32:48--  https://storage.googleapis.com/roboflow-platform-exports/pwYAXv9BTpqLyFfgQoPZ/peHMC7FkjCfPPfu0wVn3/2/voc.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=481589474394-compute%40developer.gserviceaccount.com%2F20231201%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20231201T123247Z&X-Goog-Expires=901&X-Goog-SignedHeaders=host&X-Goog-Signature=2de2cb741bb179b2c80f64e1ee4cc344f8224183ed5afbe17295a5af5c7d513bdd64ef0e421976cd01ebe0e6f914fa5f81bacb7ea66ba4c8dd2182ca7f94d06bb5fd0e0fd1a1565aae0aca758ba94feff17f5a9fe0d7c67d70d21b0d8b2d33dcbb58f9d357583e56f8fc9c495cc6963157205349393cd2a592e2178804c5d32cfa910d3f01efd13b3f518f7f68b9a8aab3ff27ce4783192632f0d5c61b2908cbd0bc8c281b8358ad590e153a25e3d653389e045f0e1e4bcaf53a2c28571707048fa9104da3c7f082a3cf07dd1072124b6caa7aa4b7281d5b8a94c613a1563300ac4f88a2c64e5cbf77ef430d3b9b4ff88f8ae9a12f8b05f9478494c2522e6e57\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.119.207, 108.177.127.207, 172.217.218.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.119.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 197871505 (189M) [application/zip]\n",
            "Saving to: ‘Zq7rUF1khY?key=UXLLVXz75H’\n",
            "\n",
            "Zq7rUF1khY?key=UXLL 100%[===================>] 188.70M  30.4MB/s    in 6.7s    \n",
            "\n",
            "2023-12-01 12:32:54 (28.1 MB/s) - ‘Zq7rUF1khY?key=UXLLVXz75H’ saved [197871505/197871505]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data count"
      ],
      "metadata": {
        "id": "w9tCHGRInKCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_images(path):\n",
        "  files = []\n",
        "  for img_anot in glob.glob(str(path)+\"/*.xml\"):\n",
        "    xml_file = minidom.parse(img_anot)\n",
        "    files.append(xml_file)\n",
        "\n",
        "  return len(files)"
      ],
      "metadata": {
        "id": "ItDRh-ZanNdF"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training images: {count_images(TRAINING_DIR)}\")\n",
        "print(f\"Validation images: {count_images(VALIDATION_DIR)}\")\n",
        "print(f\"Test images: {count_images(TEST_DIR)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y0D6NNKneyW",
        "outputId": "87349b48-47a5-4377-9f34-35938d143514"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: 2634\n",
            "Validation images: 966\n",
            "Test images: 458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate data from dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "3cpItdCOPPpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns a batch of image and labels\n",
        "def load_data(filepath, class_names, S=7, B=2,\n",
        "                  dims=[448, 448], n_channels=3,\n",
        "                  shuffle=True, transform=None):\n",
        "\n",
        "  files = []\n",
        "  class_labels = {}\n",
        "\n",
        "  for img_anot in glob.glob(str(filepath)+\"/*.xml\"):\n",
        "    xml_file = minidom.parse(img_anot)\n",
        "    files.append(xml_file)\n",
        "\n",
        "  for i, class_name in enumerate(class_names):\n",
        "    class_labels[class_name] = i\n",
        "\n",
        "  img_files = [] # (Batch, )\n",
        "  labels = [] # (Batch, S, S, C+5*B)\n",
        "\n",
        "  for i, fxml in enumerate(files):\n",
        "    # Get the image dimensions\n",
        "    img_height = dims[0]\n",
        "    img_width = dims[1]\n",
        "\n",
        "    # Get all the objects that are present in the image like car, truck, bus  etc\n",
        "    objects = fxml.getElementsByTagName('object')\n",
        "\n",
        "    boxes = []\n",
        "\n",
        "    # Process all objects by taking their name and bounding box coordinates\n",
        "    for object_ in objects:\n",
        "\n",
        "      class_name = object_.getElementsByTagName('name')[0].firstChild.data\n",
        "      # Modify class names\n",
        "      if 'truck' in class_name:\n",
        "        class_name = 'truck'\n",
        "      elif 'bus' in class_name:\n",
        "        class_name = 'bus'\n",
        "      else:\n",
        "        class_name = 'car'\n",
        "\n",
        "      class_label = class_labels[class_name]\n",
        "\n",
        "      bbox = object_.getElementsByTagName('bndbox')[0]\n",
        "      xmin = int(bbox.getElementsByTagName('xmin')[0].firstChild.data)\n",
        "      xmax = int(bbox.getElementsByTagName('xmax')[0].firstChild.data)\n",
        "      ymin = int(bbox.getElementsByTagName('ymin')[0].firstChild.data)\n",
        "      ymax = int(bbox.getElementsByTagName('ymax')[0].firstChild.data)\n",
        "\n",
        "      centerx = (xmax - (xmax-xmin)/2) / img_width\n",
        "      centery = ((ymax - (ymax-ymin)/2)) / img_height\n",
        "      boxwidth = (xmax - xmin) / img_width\n",
        "      boxheight = (ymax - ymin) / img_height\n",
        "\n",
        "      boxes.append([class_label, centerx, centery, boxwidth, boxheight])\n",
        "\n",
        "    # label matrix with (S, S, C+5*B) shape\n",
        "    label_matrix = tf.zeros([S, S, C+5*B])\n",
        "\n",
        "    for box in boxes:\n",
        "      class_label, x, y, width, height = box\n",
        "\n",
        "      class_label = int(class_label)\n",
        "\n",
        "      # grid row and column\n",
        "      row, column = int(S * y) , int(S * x)\n",
        "\n",
        "      # x and y wrt cell\n",
        "      cell_x, cell_y = (S * x - column), (S * y - row)\n",
        "\n",
        "      # width and height wrt cell\n",
        "      width_cell, height_cell = (\n",
        "          width * S,\n",
        "          height * S\n",
        "      )\n",
        "\n",
        "      if row >= S or column >= S:\n",
        "        continue # skip this object\n",
        "\n",
        "      # Check whether this cell already have a bounding box entry or not\n",
        "      # By using the confidence score value which is initially zero.\n",
        "      if label_matrix[row, column, C] == 0:\n",
        "        # Set the confidence score to 1\n",
        "        indices = [[row, column, C]]\n",
        "        updates = [1]\n",
        "        label_matrix = tf.tensor_scatter_nd_update(label_matrix, indices, updates)\n",
        "\n",
        "        # Set the class label to 1\n",
        "        indices = [[row, column, class_label]]\n",
        "        updates = [1]\n",
        "        label_matrix = tf.tensor_scatter_nd_update(label_matrix, indices, updates)\n",
        "\n",
        "        # Set the bounding box values\n",
        "        indices = [[row, column, C+1], [row, column, C+2], [row, column, C+3],\n",
        "                    [row, column, C+4]]\n",
        "        updates = [cell_x, cell_y, width_cell, height_cell]\n",
        "        label_matrix = tf.tensor_scatter_nd_update(label_matrix, indices, updates)\n",
        "\n",
        "    # Store them into images and labels array\n",
        "    img_files.append(filepath+'/'+fxml.getElementsByTagName('filename')[0].firstChild.data)\n",
        "    labels.append(label_matrix)\n",
        "\n",
        "  img_files = tf.stack(img_files, axis=0)\n",
        "  labels = tf.stack(labels, axis=0)\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((img_files, labels))\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "h0JgPNpSPvrC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(img_file, labels):\n",
        "  img = tf.io.read_file(img_file)\n",
        "  img = tf.io.decode_jpeg(img)\n",
        "  img = tf.image.convert_image_dtype(img, dtype=tf.float32)\n",
        "  img = tf.image.resize(img, [*IMAGE_DIMS])\n",
        "\n",
        "  return img, labels\n",
        "\n",
        "def tune_training_ds(dataset):\n",
        "  dataset = dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  dataset = dataset.shuffle(1024, reshuffle_each_iteration=True)\n",
        "  dataset = dataset.repeat() # The dataset be repeated indefinitely.\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "  return dataset\n",
        "\n",
        "def tune_validation_ds(dataset):\n",
        "  dataset = dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.repeat()\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "RAGy7Zw4R0zj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_ds = load_data(TRAINING_DIR, CLASS_NAMES)"
      ],
      "metadata": {
        "id": "juyrKVzuYTrk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tune_training_ds(raw_train_ds)"
      ],
      "metadata": {
        "id": "46OMZN6uYd0t"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_val_ds = load_data(TRAINING_DIR, CLASS_NAMES)"
      ],
      "metadata": {
        "id": "B-vyOV1Mhcuq"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = tune_validation_ds(raw_val_ds)"
      ],
      "metadata": {
        "id": "sK6C6BA4hhKR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "I2FF5QEWd1bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNNBlock Class\n",
        "This class implements the `convolutional neural network` along with `BatchNormalization` layer and also applies `LeakyReLU`."
      ],
      "metadata": {
        "id": "fsYmVOFNsmCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class ImageProcessor(tf.keras.layers.Layer):\n",
        "  def __init__(self, img_shape, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.img_shape = img_shape\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.model = keras.Sequential([\n",
        "        # First layer\n",
        "        tf.keras.layers.Conv2D(filters=64, kernel_size=(7, 7),\n",
        "                               padding='same', strides=(2, 2), input_shape=self.img_shape),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Second layer\n",
        "        tf.keras.layers.Conv2D(filters=192, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Third layer\n",
        "        tf.keras.layers.Conv2D(filters=128, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Fourth layer\n",
        "        tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Fifth layer\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3),\n",
        "                               padding='same', strides=(2, 2)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "        # Sixth layer\n",
        "        tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU()\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "GGmMGj_c5BNi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YoloV1 class\n",
        "This class extends `tf.keras.Model` class which is the class for building models and training them.\n",
        "\n",
        "By inheriting the `Model` class, we are making sure that our `YoloV1` class have every tensorflow details implemented in it like parallel processing, usage of GPU, and many other utilities. But, by extending we also make sure that we customize the process of training, testing and model creation."
      ],
      "metadata": {
        "id": "FLHQjaYJs_e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class YoloV1(tf.keras.Model):\n",
        "  def __init__(self, S, B, C, img_shape=[448, 448], n_channels=3,\n",
        "               name=\"YoloV1\", **kwargs):\n",
        "    super().__init__(name=name, **kwargs)\n",
        "\n",
        "    self.image_processor = ImageProcessor([*img_shape, n_channels])\n",
        "\n",
        "    self.dense_1 = tf.keras.layers.Dense(4096)\n",
        "    self.dropout = tf.keras.layers.Dropout(0.2)\n",
        "    self.leaky_relu = tf.keras.layers.LeakyReLU()\n",
        "    self.final = tf.keras.layers.Dense(B*5+C, activation='linear')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.image_processor(x)\n",
        "\n",
        "    x = self.dense_1(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.leaky_relu(x)\n",
        "    x = self.final(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def train_step(self, data):\n",
        "    if len(data) == 3:\n",
        "      x, y, sample_weight = data\n",
        "    else:\n",
        "      sample_weight = None\n",
        "      x, y = data\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred = self(x, training=True)\n",
        "      loss = self.compute_loss(y=y, y_pred=y_pred)\n",
        "\n",
        "    # Calculate gradients\n",
        "    trainable_vars = self.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "    # Update weights\n",
        "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "    for metric in self.metrics:\n",
        "      if metric.name == \"loss\":\n",
        "          metric.update_state(loss)\n",
        "      else:\n",
        "          metric.update_state(y, y_pred, sample_weight=sample_weight)\n",
        "\n",
        "    return {m.name: m.result() for m in self.metrics}"
      ],
      "metadata": {
        "id": "rnMXDgZpn8cX"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection_over_union(boxes_pred, boxes_true):\n",
        "\n",
        "  # Box 1 = [xmin, ymin, xmax, ymax]\n",
        "  box1_xmin = boxes_pred[..., 0:1] - boxes_pred[..., 2:3] / 2\n",
        "  box1_ymin = boxes_pred[..., 1:2] - boxes_pred[..., 3:4] / 2\n",
        "  box1_xmax = boxes_pred[..., 0:1] + boxes_pred[..., 2:3] / 2\n",
        "  box1_ymax = boxes_pred[..., 1:2] + boxes_pred[..., 3:4] / 2\n",
        "\n",
        "  # Box 2 = [xmin, ymin, xmax, ymax]\n",
        "  box2_xmin = boxes_true[..., 0:1] - boxes_true[..., 2:3] / 2\n",
        "  box2_ymin = boxes_true[..., 1:2] - boxes_true[..., 3:4] / 2\n",
        "  box2_xmax = boxes_true[..., 0:1] + boxes_true[..., 2:3] / 2\n",
        "  box2_ymax = boxes_true[..., 1:2] + boxes_true[..., 3:4] / 2\n",
        "\n",
        "  # [xmin, ymin, xmax, ymax] of the common area\n",
        "  common_xmin = tf.reduce_max([box1_xmin, box2_xmin], axis=-1, keepdims=True)\n",
        "  common_ymin = tf.reduce_max([box1_ymin, box2_ymin], axis=-1, keepdims=True)\n",
        "  common_xmax = tf.reduce_min([box1_xmax, box2_xmax], axis=-1, keepdims=True)\n",
        "  common_ymax = tf.reduce_min([box1_ymax, box2_ymax], axis=-1, keepdims=True)\n",
        "  # Calculate the area of the common part\n",
        "  common_area = tf.clip_by_value((common_xmax-common_xmin)*(common_ymax-common_ymin),\n",
        "                                 clip_value_min=0, clip_value_max=tf.float32.max)\n",
        "\n",
        "  # Find the union\n",
        "  union_area = ((boxes_pred[..., 2:3] * boxes_true[..., 3:4]) - common_area) + 1e-6\n",
        "\n",
        "  return common_area / union_area\n",
        "\n",
        "class YoloLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self, S, B, C, lambda_coord, lambda_noobj):\n",
        "    super(YoloLoss, self).__init__()\n",
        "    self.mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    self.S = S\n",
        "    self.B = B\n",
        "    self.C = C\n",
        "    self.lambda_coord = lambda_coord\n",
        "    self.lambda_noobj = lambda_noobj\n",
        "\n",
        "  def call(self, y_true, y_pred, sample_weights=None):\n",
        "    y_true = tf.reshape(y_true, shape=[-1, self.S, self.S, self.C+5*self.B]) # (Batch, S, S, C+5*B)\n",
        "    y_pred = tf.reshape(y_pred, shape=[-1, self.S, self.S, self.C+5*self.B]) # (Batch, S, S, C+5*B)\n",
        "\n",
        "    ## Find IOU for all bounding boxes\n",
        "    # ious = []\n",
        "    # for i in range(self.B):\n",
        "      # iou = intersection_over_union(y_pred[..., self.C+(i*5+1):self.C+(i*5+5)], y_true[..., self.C+1:self.C+5])\n",
        "      # ious.append(iou)\n",
        "\n",
        "    # Without loop, shape=>(Batch, S, S, 1)\n",
        "    iou_box1 = intersection_over_union(y_pred[..., self.C+1:self.C+5], y_true[..., self.C+1:self.C+5])\n",
        "    iou_box2 = intersection_over_union(y_pred[..., self.C+6:self.C+10], y_true[..., self.C+1:self.C+5])\n",
        "\n",
        "    ious = tf.concat([iou_box1, iou_box2], axis=0) # (2, Batch, S, S, 1)\n",
        "\n",
        "    best_boxes = tf.cast(tf.argmax(ious, axis=0), dtype=tf.float32) # (Batch, S, S, 1)\n",
        "\n",
        "    exists_box = y_true[..., self.C:self.C+1] # I_obj, shape=>(Batch, S, S, 1)\n",
        "\n",
        "    # Box loss, shape=>(Batch, S, S, 4)\n",
        "    box_predictions = best_boxes * y_pred[...,\n",
        "                                          self.C+1:self.C+5] + (1 - best_boxes) * y_pred[...,\n",
        "                                                                                          self.C+1:self.C+5]\n",
        "    box_targets = y_true[..., self.C+1:self.C+5]\n",
        "\n",
        "    # ## Find the sqrt of the width and height (4 lists of shape (Batch, S, S))\n",
        "    box_predictions = tf.unstack(box_predictions, axis=-1)\n",
        "    box_targets = tf.unstack(box_targets, axis=-1)\n",
        "\n",
        "    box_predictions[2] = tf.sign(box_predictions[2]) * tf.sqrt(tf.abs(box_predictions[2])+1e-6)\n",
        "    box_predictions[3] = tf.sign(box_predictions[3]) * tf.sqrt(tf.abs(box_predictions[3])+1e-6)\n",
        "    box_targets[2] = tf.sqrt(box_targets[2])\n",
        "    box_targets[3] = tf.sqrt(box_targets[3])\n",
        "\n",
        "    box_predictions = tf.stack(box_predictions, axis=-1)\n",
        "    box_targets = tf.stack(box_targets, axis=-1)\n",
        "\n",
        "    box_loss = self.mse(\n",
        "        tf.reshape(box_predictions, shape=[-1]),\n",
        "        tf.reshape(box_targets, shape=[-1])\n",
        "    )\n",
        "\n",
        "    # Object loss\n",
        "    pred_box = (best_boxes) * y_pred[..., self.C:self.C+1] + (1 - best_boxes) * y_pred[..., self.C+5:self.C+6]\n",
        "\n",
        "    object_loss = self.mse(tf.reshape(exists_box * pred_box, shape=[-1]),\n",
        "                           tf.reshape(exists_box * y_true[..., self.C:self.C+1], shape=[-1]))\n",
        "\n",
        "    # No object loss\n",
        "    noobj_loss = self.mse(\n",
        "        tf.reshape((1 - exists_box) * y_pred[..., self.C:self.C+1], shape=[-1]),\n",
        "        tf.reshape((1 - exists_box) * y_true[..., self.C:self.C+1], shape=[-1])\n",
        "    )\n",
        "\n",
        "    noobj_loss += self.mse(\n",
        "        tf.reshape((1 - exists_box) * y_pred[..., self.C+5:self.C+6], shape=[-1]),\n",
        "        tf.reshape((1 - exists_box) * y_true[..., self.C:self.C+1], shape=[-1])\n",
        "    )\n",
        "\n",
        "    # Class loss\n",
        "    class_loss = self.mse(\n",
        "        tf.reshape(exists_box * y_pred[..., :self.C], shape=[-1]),\n",
        "        tf.reshape(exists_box * y_true[..., :self.C], shape=[-1])\n",
        "    )\n",
        "\n",
        "    # Total loss\n",
        "    loss = (\n",
        "        self.lambda_coord * box_loss +\n",
        "        self.lambda_coord * object_loss +\n",
        "        object_loss +\n",
        "        self.lambda_noobj * noobj_loss +\n",
        "        class_loss\n",
        "    )\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "I0bLSFneX_xR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MeanAveragePrecision(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name='mean_average_precision', **kwargs):\n",
        "    super(MeanAveragePrecision, self).__init__(name=name, **kwargs)\n",
        "    self.mean_avg_precision = self.add_weight(name='map', initializer='zeros')\n",
        "\n",
        "  def update_state(self, y_pred, y_true, sample_weight=None):\n",
        "    y_true = tf.reshape(y_true, shape=[-1, self.S, self.S, self.C+5*self.B]) # (Batch, S, S, C+5*B)\n",
        "    y_pred = tf.reshape(y_pred, shape=[-1, self.S, self.S, self.C+5*self.B]) # (Batch, S, S, C+5*B)\n",
        "\n",
        "\n",
        "\n",
        "  def result(self):\n",
        "    return self.mean_avg_precision"
      ],
      "metadata": {
        "id": "RABBS2s3pbDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "yolo = YoloV1(**model_params)\n",
        "\n",
        "# Compile the model\n",
        "yolo.compile(optimizer='rmsprop',\n",
        "             loss=YoloLoss(**loss_params),\n",
        "             metrics=[MeanAveragePrecision()])\n",
        "\n",
        "# Fit on the dataset\n",
        "yolo.fit(train_ds,\n",
        "         epochs=EPOCHS,\n",
        "         steps_per_epoch=TRAIN_STEPS,\n",
        "         validation_data=val_ds,\n",
        "         validation_steps=VAL_STEPS)"
      ],
      "metadata": {
        "id": "3aA0KJn3IqY9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}