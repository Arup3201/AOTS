{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMSUxJKuK+9UFt5rhJjx0LH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arup3201/AOTS/blob/main/notebooks/Artificial_Traffic_Management_ML_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Traffic Management - ML Model"
      ],
      "metadata": {
        "id": "UjP0jDMnOEUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "AtqH5ZCdPBUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPfj3SEzPHtv",
        "outputId": "f3c1961b-f618-4184-d86e-af31329fc762"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import image\n",
        "from xml.dom import minidom\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "PffOpxnJPIeT"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variables**"
      ],
      "metadata": {
        "id": "4M-2axWNQVGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_DIR = '/content/dataset/train'\n",
        "VALIDATION_DIR = '/content/dataset/valid'\n",
        "TEST_DIR = '/content/dataset/test'\n",
        "CLASS_NAMES = ['car', 'bus', 'truck']\n",
        "S = 7\n",
        "B = 2\n",
        "C = len(CLASS_NAMES)\n",
        "IMAGE_DIMS = [448, 448]\n",
        "NUM_CHANNELS = 3\n",
        "BATCH_SIZE = 16\n",
        "LAMBDA_COORD = 5\n",
        "LAMBDA_NOOBJ = 0.5\n",
        "\n",
        "EPOCHS = 1\n",
        "TRAIN_STEPS = 160\n",
        "VAL_STEPS = 60\n",
        "\n",
        "dataset_params = {'class_names': CLASS_NAMES,\n",
        "                  'S': S,\n",
        "                  'B': B,\n",
        "                  'dims': IMAGE_DIMS,\n",
        "                  'n_channels': NUM_CHANNELS,\n",
        "                  'batch_size': BATCH_SIZE,\n",
        "                  'shuffle': True,\n",
        "                  'transform': None}\n",
        "\n",
        "model_params = {'S': S,\n",
        "                'B': B,\n",
        "                'C': C,\n",
        "                'img_shape': IMAGE_DIMS}\n",
        "\n",
        "loss_params = {'S': S,\n",
        "               'B': B,\n",
        "               'C': C,\n",
        "               'lambda_coord': LAMBDA_COORD,\n",
        "               'lambda_noobj': LAMBDA_NOOBJ}"
      ],
      "metadata": {
        "id": "sBHeG1PiQZLu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "FZULTkTfPL5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Data"
      ],
      "metadata": {
        "id": "v5Uvgo76PnwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r dataset/\n",
        "!wget https://universe.roboflow.com/ds/Zq7rUF1khY?key=UXLLVXz75H\n",
        "\n",
        "!mkdir dataset\n",
        "!unzip -q Zq7rUF1khY?key=UXLLVXz75H -d dataset/\n",
        "!rm Zq7rUF1khY?key=UXLLVXz75H"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhNvtVWNPPDN",
        "outputId": "33b5e7f6-a51e-4959-c611-dff825425e6d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'dataset/': No such file or directory\n",
            "--2023-12-02 02:33:58--  https://universe.roboflow.com/ds/Zq7rUF1khY?key=UXLLVXz75H\n",
            "Resolving universe.roboflow.com (universe.roboflow.com)... 151.101.65.195, 151.101.1.195\n",
            "Connecting to universe.roboflow.com (universe.roboflow.com)|151.101.65.195|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://storage.googleapis.com/roboflow-platform-exports/pwYAXv9BTpqLyFfgQoPZ/peHMC7FkjCfPPfu0wVn3/2/voc.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=481589474394-compute%40developer.gserviceaccount.com%2F20231202%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20231202T023358Z&X-Goog-Expires=901&X-Goog-SignedHeaders=host&X-Goog-Signature=526086dc16ab05193cfc3eb1445d02b335b13e164aedf8e1b8ce8549882d9606d0ff0ec309db2149547a2e7a0e54343ae0d631e8b92ed117df8049094d07b99bd26d1d101ab30eb72f8c931c7d2e6add58081c1ad243c70aca836bdec00ffb961331c413fd8b03e8a0f9051347629015add629360f6ea88c99980197adb23cc1bda2d4c15eebf23b22293c5caa83e583c10deca14a2357e50113484541d329b033d11de2f21200a67efc367a666c2d8af3a16c6407f8662c12042f8e481fb18ef42940bb8e0829d5b64983fa571d04ea7fb4ae7432c3ca8affd32402dee8a23cb39dca33d1cd2991973ea32f3705900e47dd94eb2983faca8414de421d99e58f [following]\n",
            "--2023-12-02 02:33:58--  https://storage.googleapis.com/roboflow-platform-exports/pwYAXv9BTpqLyFfgQoPZ/peHMC7FkjCfPPfu0wVn3/2/voc.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=481589474394-compute%40developer.gserviceaccount.com%2F20231202%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20231202T023358Z&X-Goog-Expires=901&X-Goog-SignedHeaders=host&X-Goog-Signature=526086dc16ab05193cfc3eb1445d02b335b13e164aedf8e1b8ce8549882d9606d0ff0ec309db2149547a2e7a0e54343ae0d631e8b92ed117df8049094d07b99bd26d1d101ab30eb72f8c931c7d2e6add58081c1ad243c70aca836bdec00ffb961331c413fd8b03e8a0f9051347629015add629360f6ea88c99980197adb23cc1bda2d4c15eebf23b22293c5caa83e583c10deca14a2357e50113484541d329b033d11de2f21200a67efc367a666c2d8af3a16c6407f8662c12042f8e481fb18ef42940bb8e0829d5b64983fa571d04ea7fb4ae7432c3ca8affd32402dee8a23cb39dca33d1cd2991973ea32f3705900e47dd94eb2983faca8414de421d99e58f\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.175.207, 74.125.24.207, 142.250.4.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.175.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 197871505 (189M) [application/zip]\n",
            "Saving to: ‘Zq7rUF1khY?key=UXLLVXz75H’\n",
            "\n",
            "Zq7rUF1khY?key=UXLL 100%[===================>] 188.70M  20.1MB/s    in 11s     \n",
            "\n",
            "2023-12-02 02:34:10 (16.9 MB/s) - ‘Zq7rUF1khY?key=UXLLVXz75H’ saved [197871505/197871505]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data count"
      ],
      "metadata": {
        "id": "w9tCHGRInKCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_images(path):\n",
        "  files = []\n",
        "  for img_anot in glob.glob(str(path)+\"/*.xml\"):\n",
        "    xml_file = minidom.parse(img_anot)\n",
        "    files.append(xml_file)\n",
        "\n",
        "  return len(files)"
      ],
      "metadata": {
        "id": "ItDRh-ZanNdF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training images: {count_images(TRAINING_DIR)}\")\n",
        "print(f\"Validation images: {count_images(VALIDATION_DIR)}\")\n",
        "print(f\"Test images: {count_images(TEST_DIR)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y0D6NNKneyW",
        "outputId": "328d6a15-e3f4-4f0d-8098-1ffb3e3e60ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: 2634\n",
            "Validation images: 966\n",
            "Test images: 458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate data from dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "3cpItdCOPPpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Returns a batch of image and labels\n",
        "def load_data(filepath, class_names, S=7, B=2,\n",
        "                  dims=[448, 448], n_channels=3,\n",
        "                  shuffle=True, transform=None):\n",
        "\n",
        "  files = []\n",
        "  class_labels = {}\n",
        "\n",
        "  for img_anot in glob.glob(str(filepath)+\"/*.xml\"):\n",
        "    xml_file = minidom.parse(img_anot)\n",
        "    files.append(xml_file)\n",
        "\n",
        "  for i, class_name in enumerate(class_names):\n",
        "    class_labels[class_name] = i\n",
        "\n",
        "  img_files = [] # (Batch, )\n",
        "  labels = [] # (Batch, S, S, C+5*B)\n",
        "\n",
        "  for i, fxml in enumerate(files):\n",
        "    # Get the image dimensions\n",
        "    img_height = dims[0]\n",
        "    img_width = dims[1]\n",
        "\n",
        "    # Get all the objects that are present in the image like car, truck, bus  etc\n",
        "    objects = fxml.getElementsByTagName('object')\n",
        "\n",
        "    boxes = []\n",
        "\n",
        "    # Process all objects by taking their name and bounding box coordinates\n",
        "    for object_ in objects:\n",
        "\n",
        "      class_name = object_.getElementsByTagName('name')[0].firstChild.data\n",
        "      # Modify class names\n",
        "      if 'truck' in class_name:\n",
        "        class_name = 'truck'\n",
        "      elif 'bus' in class_name:\n",
        "        class_name = 'bus'\n",
        "      else:\n",
        "        class_name = 'car'\n",
        "\n",
        "      class_label = class_labels[class_name]\n",
        "\n",
        "      bbox = object_.getElementsByTagName('bndbox')[0]\n",
        "      xmin = int(bbox.getElementsByTagName('xmin')[0].firstChild.data)\n",
        "      xmax = int(bbox.getElementsByTagName('xmax')[0].firstChild.data)\n",
        "      ymin = int(bbox.getElementsByTagName('ymin')[0].firstChild.data)\n",
        "      ymax = int(bbox.getElementsByTagName('ymax')[0].firstChild.data)\n",
        "\n",
        "      centerx = (xmax - (xmax-xmin)/2) / img_width\n",
        "      centery = ((ymax - (ymax-ymin)/2)) / img_height\n",
        "      boxwidth = (xmax - xmin) / img_width\n",
        "      boxheight = (ymax - ymin) / img_height\n",
        "\n",
        "      boxes.append([class_label, centerx, centery, boxwidth, boxheight])\n",
        "\n",
        "    # label matrix with (S, S, C+5*B) shape\n",
        "    label_matrix = tf.zeros([S, S, C+5*B])\n",
        "\n",
        "    for box in boxes:\n",
        "      class_label, x, y, width, height = box\n",
        "\n",
        "      class_label = int(class_label)\n",
        "\n",
        "      # grid row and column\n",
        "      row, column = int(S * y) , int(S * x)\n",
        "\n",
        "      # x and y wrt cell\n",
        "      cell_x, cell_y = (S * x - column), (S * y - row)\n",
        "\n",
        "      # width and height wrt cell\n",
        "      width_cell, height_cell = (\n",
        "          width * S,\n",
        "          height * S\n",
        "      )\n",
        "\n",
        "      if row >= S or column >= S:\n",
        "        continue # skip this object\n",
        "\n",
        "      # Check whether this cell already have a bounding box entry or not\n",
        "      # By using the confidence score value which is initially zero.\n",
        "      if label_matrix[row, column, C] == 0:\n",
        "        # Set the confidence score to 1\n",
        "        indices = [[row, column, C]]\n",
        "        updates = [1]\n",
        "        label_matrix = tf.tensor_scatter_nd_update(label_matrix, indices, updates)\n",
        "\n",
        "        # Set the class label to 1\n",
        "        indices = [[row, column, class_label]]\n",
        "        updates = [1]\n",
        "        label_matrix = tf.tensor_scatter_nd_update(label_matrix, indices, updates)\n",
        "\n",
        "        # Set the bounding box values\n",
        "        indices = [[row, column, C+1], [row, column, C+2], [row, column, C+3],\n",
        "                    [row, column, C+4]]\n",
        "        updates = [cell_x, cell_y, width_cell, height_cell]\n",
        "        label_matrix = tf.tensor_scatter_nd_update(label_matrix, indices, updates)\n",
        "\n",
        "    # Store them into images and labels array\n",
        "    img_files.append(filepath+'/'+fxml.getElementsByTagName('filename')[0].firstChild.data)\n",
        "    labels.append(label_matrix)\n",
        "\n",
        "  img_files = tf.stack(img_files, axis=0)\n",
        "  labels = tf.stack(labels, axis=0)\n",
        "\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((img_files, labels))\n",
        "\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "h0JgPNpSPvrC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(img_file, labels):\n",
        "  img = tf.io.read_file(img_file)\n",
        "  img = tf.io.decode_jpeg(img)\n",
        "  img = tf.image.convert_image_dtype(img, dtype=tf.float32)\n",
        "  img = tf.image.resize(img, [*IMAGE_DIMS])\n",
        "\n",
        "  return img, labels\n",
        "\n",
        "def tune_training_ds(dataset):\n",
        "  dataset = dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  dataset = dataset.shuffle(1024, reshuffle_each_iteration=True)\n",
        "  dataset = dataset.repeat() # The dataset be repeated indefinitely.\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "  return dataset\n",
        "\n",
        "def tune_validation_ds(dataset):\n",
        "  dataset = dataset.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "  dataset = dataset.batch(BATCH_SIZE)\n",
        "  dataset = dataset.repeat()\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "RAGy7Zw4R0zj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_ds = load_data(TRAINING_DIR, CLASS_NAMES)"
      ],
      "metadata": {
        "id": "juyrKVzuYTrk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tune_training_ds(raw_train_ds)"
      ],
      "metadata": {
        "id": "46OMZN6uYd0t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_val_ds = load_data(TRAINING_DIR, CLASS_NAMES)"
      ],
      "metadata": {
        "id": "B-vyOV1Mhcuq"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = tune_validation_ds(raw_val_ds)"
      ],
      "metadata": {
        "id": "sK6C6BA4hhKR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "I2FF5QEWd1bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNNBlock Class\n",
        "This class implements the `convolutional neural network` along with `BatchNormalization` layer and also applies `LeakyReLU`."
      ],
      "metadata": {
        "id": "fsYmVOFNsmCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class ImageProcessor(tf.keras.layers.Layer):\n",
        "  def __init__(self, img_shape, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.img_shape = img_shape\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.model = keras.Sequential([\n",
        "        # First layer\n",
        "        tf.keras.layers.Conv2D(filters=64, kernel_size=(7, 7),\n",
        "                               padding='same', strides=(2, 2), input_shape=self.img_shape),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Second layer\n",
        "        tf.keras.layers.Conv2D(filters=192, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Third layer\n",
        "        tf.keras.layers.Conv2D(filters=128, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=256, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Fourth layer\n",
        "        tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=256, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2)),\n",
        "\n",
        "        # Fifth layer\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=512, kernel_size=(1, 1),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3),\n",
        "                               padding='same', strides=(2, 2)),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "        # Sixth layer\n",
        "        tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU(),\n",
        "        tf.keras.layers.Conv2D(filters=1024, kernel_size=(3, 3),\n",
        "                               padding='same'),\n",
        "        tf.keras.layers.BatchNormalization(),\n",
        "        tf.keras.layers.LeakyReLU()\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "GGmMGj_c5BNi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### YoloV1 class\n",
        "This class extends `tf.keras.Model` class which is the class for building models and training them.\n",
        "\n",
        "By inheriting the `Model` class, we are making sure that our `YoloV1` class have every tensorflow details implemented in it like parallel processing, usage of GPU, and many other utilities. But, by extending we also make sure that we customize the process of training, testing and model creation."
      ],
      "metadata": {
        "id": "FLHQjaYJs_e0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@keras.saving.register_keras_serializable()\n",
        "class YoloV1(tf.keras.Model):\n",
        "  def __init__(self, S, B, C, img_shape=[448, 448], n_channels=3,\n",
        "               name=\"YoloV1\", **kwargs):\n",
        "    super().__init__(name=name, **kwargs)\n",
        "\n",
        "    self.image_processor = ImageProcessor([*img_shape, n_channels])\n",
        "\n",
        "    self.dense_1 = tf.keras.layers.Dense(4096)\n",
        "    self.dropout = tf.keras.layers.Dropout(0.2)\n",
        "    self.leaky_relu = tf.keras.layers.LeakyReLU()\n",
        "    self.final = tf.keras.layers.Dense(B*5+C, activation='linear')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.image_processor(x)\n",
        "\n",
        "    x = self.dense_1(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.leaky_relu(x)\n",
        "    x = self.final(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "  def train_step(self, data):\n",
        "    if len(data) == 3:\n",
        "      x, y, sample_weight = data\n",
        "    else:\n",
        "      sample_weight = None\n",
        "      x, y = data\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred = self(x, training=True)\n",
        "      loss = self.compute_loss(y=y, y_pred=y_pred)\n",
        "\n",
        "    # Calculate gradients\n",
        "    trainable_vars = self.trainable_variables\n",
        "    gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "    # Update weights\n",
        "    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "    for metric in self.metrics:\n",
        "      if metric.name == \"loss\":\n",
        "          metric.update_state(loss)\n",
        "      else:\n",
        "          metric.update_state(y, y_pred, sample_weight=sample_weight)\n",
        "\n",
        "    return {m.name: m.result() for m in self.metrics}"
      ],
      "metadata": {
        "id": "rnMXDgZpn8cX"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection_over_union(boxes_pred, boxes_true):\n",
        "\n",
        "  # Box 1 = [xmin, ymin, xmax, ymax]\n",
        "  box1_xmin = boxes_pred[..., 0:1] - boxes_pred[..., 2:3] / 2\n",
        "  box1_ymin = boxes_pred[..., 1:2] - boxes_pred[..., 3:4] / 2\n",
        "  box1_xmax = boxes_pred[..., 0:1] + boxes_pred[..., 2:3] / 2\n",
        "  box1_ymax = boxes_pred[..., 1:2] + boxes_pred[..., 3:4] / 2\n",
        "\n",
        "  # Box 2 = [xmin, ymin, xmax, ymax]\n",
        "  box2_xmin = boxes_true[..., 0:1] - boxes_true[..., 2:3] / 2\n",
        "  box2_ymin = boxes_true[..., 1:2] - boxes_true[..., 3:4] / 2\n",
        "  box2_xmax = boxes_true[..., 0:1] + boxes_true[..., 2:3] / 2\n",
        "  box2_ymax = boxes_true[..., 1:2] + boxes_true[..., 3:4] / 2\n",
        "\n",
        "  # [xmin, ymin, xmax, ymax] of the common area\n",
        "  common_xmin = tf.reduce_max([box1_xmin, box2_xmin], axis=-1, keepdims=True)\n",
        "  common_ymin = tf.reduce_max([box1_ymin, box2_ymin], axis=-1, keepdims=True)\n",
        "  common_xmax = tf.reduce_min([box1_xmax, box2_xmax], axis=-1, keepdims=True)\n",
        "  common_ymax = tf.reduce_min([box1_ymax, box2_ymax], axis=-1, keepdims=True)\n",
        "  # Calculate the area of the common part\n",
        "  common_area = tf.clip_by_value((common_xmax-common_xmin)*(common_ymax-common_ymin),\n",
        "                                 clip_value_min=0, clip_value_max=tf.float32.max)\n",
        "\n",
        "  # Find the union\n",
        "  union_area = ((boxes_pred[..., 2:3] * boxes_true[..., 3:4]) - common_area) + 1e-6\n",
        "\n",
        "  return common_area / union_area\n",
        "\n",
        "class YoloLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self, S, B, C, lambda_coord, lambda_noobj):\n",
        "    super(YoloLoss, self).__init__()\n",
        "    self.mse = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "    self.S = S\n",
        "    self.B = B\n",
        "    self.C = C\n",
        "    self.lambda_coord = lambda_coord\n",
        "    self.lambda_noobj = lambda_noobj\n",
        "\n",
        "  def call(self, y_true, y_pred, sample_weights=None):\n",
        "    y_true = tf.reshape(y_true, shape=[-1, self.S, self.S, self.C+5*self.B]) # (Batch, S, S, C+5*B)\n",
        "    y_pred = tf.reshape(y_pred, shape=[-1, self.S, self.S, self.C+5*self.B]) # (Batch, S, S, C+5*B)\n",
        "\n",
        "    ## Find IOU for all bounding boxes\n",
        "    # ious = []\n",
        "    # for i in range(self.B):\n",
        "      # iou = intersection_over_union(y_pred[..., self.C+(i*5+1):self.C+(i*5+5)], y_true[..., self.C+1:self.C+5])\n",
        "      # ious.append(iou)\n",
        "\n",
        "    # Without loop, shape=>(Batch, S, S, 1)\n",
        "    iou_box1 = intersection_over_union(y_pred[..., self.C+1:self.C+5], y_true[..., self.C+1:self.C+5])\n",
        "    iou_box2 = intersection_over_union(y_pred[..., self.C+6:self.C+10], y_true[..., self.C+1:self.C+5])\n",
        "\n",
        "    ious = tf.concat([iou_box1, iou_box2], axis=0) # (2, Batch, S, S, 1)\n",
        "\n",
        "    best_boxes = tf.cast(tf.argmax(ious, axis=0), dtype=tf.float32) # (Batch, S, S, 1)\n",
        "\n",
        "    exists_box = y_true[..., self.C:self.C+1] # I_obj, shape=>(Batch, S, S, 1)\n",
        "\n",
        "    # Box loss, shape=>(Batch, S, S, 4)\n",
        "    box_predictions = best_boxes * y_pred[...,\n",
        "                                          self.C+1:self.C+5] + (1 - best_boxes) * y_pred[...,\n",
        "                                                                                          self.C+1:self.C+5]\n",
        "    box_targets = y_true[..., self.C+1:self.C+5]\n",
        "\n",
        "    # ## Find the sqrt of the width and height (4 lists of shape (Batch, S, S))\n",
        "    box_predictions = tf.unstack(box_predictions, axis=-1)\n",
        "    box_targets = tf.unstack(box_targets, axis=-1)\n",
        "\n",
        "    box_predictions[2] = tf.sign(box_predictions[2]) * tf.sqrt(tf.abs(box_predictions[2])+1e-6)\n",
        "    box_predictions[3] = tf.sign(box_predictions[3]) * tf.sqrt(tf.abs(box_predictions[3])+1e-6)\n",
        "    box_targets[2] = tf.sqrt(box_targets[2])\n",
        "    box_targets[3] = tf.sqrt(box_targets[3])\n",
        "\n",
        "    box_predictions = tf.stack(box_predictions, axis=-1)\n",
        "    box_targets = tf.stack(box_targets, axis=-1)\n",
        "\n",
        "    box_loss = self.mse(\n",
        "        tf.reshape(box_predictions, shape=[-1]),\n",
        "        tf.reshape(box_targets, shape=[-1])\n",
        "    )\n",
        "\n",
        "    # Object loss\n",
        "    pred_box = (best_boxes) * y_pred[..., self.C:self.C+1] + (1 - best_boxes) * y_pred[..., self.C+5:self.C+6]\n",
        "\n",
        "    object_loss = self.mse(tf.reshape(exists_box * pred_box, shape=[-1]),\n",
        "                           tf.reshape(exists_box * y_true[..., self.C:self.C+1], shape=[-1]))\n",
        "\n",
        "    # No object loss\n",
        "    noobj_loss = self.mse(\n",
        "        tf.reshape((1 - exists_box) * y_pred[..., self.C:self.C+1], shape=[-1]),\n",
        "        tf.reshape((1 - exists_box) * y_true[..., self.C:self.C+1], shape=[-1])\n",
        "    )\n",
        "\n",
        "    noobj_loss += self.mse(\n",
        "        tf.reshape((1 - exists_box) * y_pred[..., self.C+5:self.C+6], shape=[-1]),\n",
        "        tf.reshape((1 - exists_box) * y_true[..., self.C:self.C+1], shape=[-1])\n",
        "    )\n",
        "\n",
        "    # Class loss\n",
        "    class_loss = self.mse(\n",
        "        tf.reshape(exists_box * y_pred[..., :self.C], shape=[-1]),\n",
        "        tf.reshape(exists_box * y_true[..., :self.C], shape=[-1])\n",
        "    )\n",
        "\n",
        "    # Total loss\n",
        "    loss = (\n",
        "        self.lambda_coord * box_loss +\n",
        "        self.lambda_coord * object_loss +\n",
        "        object_loss +\n",
        "        self.lambda_noobj * noobj_loss +\n",
        "        class_loss\n",
        "    )\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "I0bLSFneX_xR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MeanAveragePrecision(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name='mean_average_precision', **kwargs):\n",
        "    super(MeanAveragePrecision, self).__init__(name=name, **kwargs)\n",
        "    self.mean_avg_precision = self.add_weight(name='map', initializer='zeros')\n",
        "\n",
        "  def update_state(self, pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20,\n",
        "                   sample_weights=None):\n",
        "    # list storing all AP for respective classes\n",
        "    average_precisions = []\n",
        "\n",
        "    # used for numerical stability later on\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for c in range(num_classes):\n",
        "      detections = []\n",
        "      ground_truths = []\n",
        "\n",
        "      # Go through all predictions and targets,\n",
        "      # and only add the ones that belong to the\n",
        "      # current class c\n",
        "      for detection in pred_boxes:\n",
        "          if detection[1] == c:\n",
        "              detections.append(detection)\n",
        "\n",
        "      for true_box in true_boxes:\n",
        "          if true_box[1] == c:\n",
        "              ground_truths.append(true_box)\n",
        "\n",
        "      # find the amount of bboxes for each training example\n",
        "      # Counter here finds how many ground truth bboxes we get\n",
        "      # for each training example, so let's say img 0 has 3,\n",
        "      # img 1 has 5 then we will obtain a dictionary with:\n",
        "      # amount_bboxes = {0:3, 1:5}\n",
        "      amount_bboxes = {key: tf.zeros(val) for key, val in np.unique([gt[0] for gt in ground_truths], return_counts=True)}\n",
        "\n",
        "      # sort by box probabilities which is index 2\n",
        "      detections.sort(key=lambda x: x[2], reverse=True)\n",
        "      TP = tf.zeros((len(detections)))\n",
        "      FP = tf.zeros((len(detections)))\n",
        "      total_true_bboxes = len(ground_truths)\n",
        "\n",
        "      # If none exists for this class then we can safely skip\n",
        "      if total_true_bboxes == 0:\n",
        "          continue\n",
        "\n",
        "      for detection_idx, detection in enumerate(detections):\n",
        "          # Only take out the ground_truths that have the same\n",
        "          # training idx as detection\n",
        "          ground_truth_img = [\n",
        "              bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
        "          ]\n",
        "\n",
        "          num_gts = len(ground_truth_img)\n",
        "          best_iou = 0\n",
        "\n",
        "          for idx, gt in enumerate(ground_truth_img):\n",
        "              iou = intersection_over_union(\n",
        "                  tf.constant(detection[3:]),\n",
        "                  tf.constant(gt[3:]),\n",
        "                  box_format=box_format,\n",
        "              )\n",
        "\n",
        "              if iou > best_iou:\n",
        "                  best_iou = iou\n",
        "                  best_gt_idx = idx\n",
        "\n",
        "          if best_iou > iou_threshold:\n",
        "              # only detect ground truth detection once\n",
        "              if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "                  # true positive and add this bounding box to seen\n",
        "                  TP[detection_idx] = 1\n",
        "                  amount_bboxes[detection[0]][best_gt_idx] = 1\n",
        "              else:\n",
        "                  FP[detection_idx] = 1\n",
        "\n",
        "          # if IOU is lower then the detection is a false positive\n",
        "          else:\n",
        "              FP[detection_idx] = 1\n",
        "\n",
        "      TP_cumsum = tf.cumsum(TP)\n",
        "      FP_cumsum = tf.cumsum(FP)\n",
        "      recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "      precisions = tf.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
        "      precisions = tf.concat([tf.constant([1.0]), precisions], axis=0)\n",
        "      recalls = tf.concat([tf.constant([0.0]), recalls], axis=0)\n",
        "      # tf.trapezoid for numerical integration\n",
        "      average_precisions.append(tf.trapezoid(precisions, recalls))\n",
        "\n",
        "    self.mean_avg_precision.assign_add(tf.reduce_sum(average_precisions) / len(average_precisions))\n",
        "\n",
        "  def result(self):\n",
        "    return self.mean_avg_precision"
      ],
      "metadata": {
        "id": "RABBS2s3pbDi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "yolo = YoloV1(**model_params)\n",
        "\n",
        "# Compile the model\n",
        "yolo.compile(optimizer='rmsprop',\n",
        "             loss=YoloLoss(**loss_params),\n",
        "             metrics=[MeanAveragePrecision()])\n",
        "\n",
        "# Fit on the dataset\n",
        "yolo.fit(train_ds,\n",
        "         epochs=EPOCHS,\n",
        "         steps_per_epoch=TRAIN_STEPS,\n",
        "         validation_data=val_ds,\n",
        "         validation_steps=VAL_STEPS)"
      ],
      "metadata": {
        "id": "3aA0KJn3IqY9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}